{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eb76a7",
   "metadata": {},
   "source": [
    "# Homework 1 - Part C\n",
    "\n",
    "Your name:\n",
    "\n",
    "Email (one used on the moodle): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e19e8",
   "metadata": {},
   "source": [
    "## Optimization problem\n",
    "\n",
    "We consider the ordinary least-squares problem (studied in the course) with the following notations:\n",
    "\\begin{eqnarray*}\n",
    "A\\in \\mathbb{R}^{n\\times d}, \\quad x^* = 1\\in \\mathbb{R}^d, \\quad b = Ax^*\\\\\n",
    "f(x) =\\frac{1}{2}\\| Ax-b\\|^2.\n",
    "\\end{eqnarray*}\n",
    "Least-squares estimation amounts to finding a minimizer of $f$: $f^* =\\min_x f(x) =0$.\n",
    "\n",
    "In the course, we analyzed the gradient descent algorithm. Here we have:\n",
    "\\begin{eqnarray*}\n",
    "\\nabla f(x) = A^T(Ax-b),\n",
    "\\end{eqnarray*}\n",
    "so that we can write the algorithm as:\n",
    "\\begin{eqnarray*}\n",
    "x_0 &=& 0\\\\\n",
    "x_{k+1} &=& x_k-\\gamma A^T(Ax_k-b).\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef070d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng   = np.random.default_rng(1)\n",
    "n   = 55\n",
    "d   = 50\n",
    "A   = rng.normal( size=(n,d) )\n",
    "xstar = np.ones( (d,1) )\n",
    "b   = A@xstar\n",
    "\n",
    "f   = lambda x : norm(A@x-b)**2/2\n",
    "grad= lambda x : A.T@( A@x-b )\n",
    "\n",
    "fstar   = 0\n",
    "x0      = np.zeros((d,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da510809",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Below is the corresponding code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b39927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(f,grad,stepsize,x0,maxiter=1e3):\n",
    "    x = x0.copy()\n",
    "    xhist = []\n",
    "    for k in range(int(maxiter)):\n",
    "        x -= stepsize*grad(x)\n",
    "        xhist.append( norm(x-xstar) )\n",
    "    return x, xhist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9824",
   "metadata": {},
   "source": [
    "In the course, we showed the closed form expression:\n",
    "\\begin{eqnarray*}\n",
    "x_{k+1} - x^* &=& x_k-x^* -\\gamma A^T A(x_k-x^*)\\\\\n",
    "x_{k+1} - x^* &=& \\left( I- \\gamma A^TA \\right)(x_k-x^*)\\\\\n",
    "&=& \\left( I- \\gamma H \\right)^k(x_0-x^*).\n",
    "\\end{eqnarray*}\n",
    "The Hessian matrix $H = A^TA$ play a crucial role. $H$ belongs to the set of $d\\times d$ symmetric matrices $\\mathcal{S}_d$. We define $\\mu=\\lambda_{\\min}(H)$ and $L=\\lambda_\\max(H)$ so that $\\kappa=\\frac{L}{\\mu}$ is the condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = np.linalg.eigvals(A.T@A)\n",
    "L     = np.max(evals)\n",
    "mu    = np.min(evals)\n",
    "kappa = L/mu\n",
    "print(f\"L is {L:.2f}, mu is {mu:.2f}, condition number is {kappa:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063a73",
   "metadata": {},
   "source": [
    "In order to maximize the speed of the algorithm, we choose $\\gamma$ to minimize the eigenvalues of $I- \\gamma A^TA$, i.e. we minimize $\\max_{\\lambda\\in[\\mu,L]}|1-\\gamma \\lambda|$. We showed that the optimal value for $\\gamma$ is $\\gamma = \\frac{2}{L+\\mu}$. Another possible weaker choice is $\\gamma =\\frac{1}{L}$ ensuring that the maximum eigenvalue is still strictly less than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 1e4\n",
    "x_gd, xhist_gd  = gradientDescent(f,grad,1/L,x0,maxiter=maxiter)\n",
    "\n",
    "x_gdopti, xhist_gdopti  = gradientDescent(f,grad,2/(L+mu),x0,maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa91665",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.semilogy( xhist_gd, label='Gradient Descent (Naive)' )\n",
    "plt.semilogy( xhist_gdopti, label='Gradient Descent (Opti)' )\n",
    "k = np.arange(1,maxiter)\n",
    "plt.semilogy(k,(1-1/kappa)**k,':',label='$(1-\\kappa^{-1})^{k}$')\n",
    "plt.semilogy(k,((kappa-1)/(kappa+1))**k,':',label='$((\\kappa-1)/(\\kappa+1))^{k}$')\n",
    "plt.legend( loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63de25f",
   "metadata": {},
   "source": [
    "## Chebyshev Acceleration\n",
    "\n",
    "Using the matrix polynomial: $P^{\\text{Grad}}_k(H) = (I-\\gamma H)^k$, the gradient descent algorithm can be written as:\n",
    "\\begin{eqnarray*}\n",
    "x_k-x^* = P^{\\text{Grad}}_k(H)(x_0-x^*),\n",
    "\\end{eqnarray*}\n",
    "and we choose $\\gamma$ to ensure $\\|I-\\gamma H\\|_2<1$.\n",
    "\n",
    "Here, we explore what happens if we can choose a sequence of polynomials in order to speed up the algorithm. The resulting algorithm will be written as:\n",
    "\\begin{eqnarray*}\n",
    "x_k-x^* = P_k(H)(x_0-x^*).\n",
    "\\end{eqnarray*}\n",
    "We cannot choose any sequence of polynomials, because we have only access to the gradients of our function evaluated at the points $x_0,\\dots, x_{k-1}$ in order to compute $x_{k}$.\n",
    "\n",
    "### Question 1 (Math)\n",
    "\n",
    "Show that if for all $k\\geq 0$, there exist some sequence of coefficients $\\{\\alpha_i^{(k)}\\}_{i=0\\dots k}$ such that\n",
    "\\begin{eqnarray*}\n",
    "x_k-x_0 = \\sum_{i=0}^{k-1} \\alpha^{(k-1)}_i \\nabla f(x_i)\n",
    "\\end{eqnarray*}\n",
    "then \n",
    "\\begin{eqnarray*}\n",
    "x_k-x^* = P_k(H)(x_0-x^*),\n",
    "\\end{eqnarray*}\n",
    "with $P_k$ of degree at most $k$ and with $P(0)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe061ff1",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f075b",
   "metadata": {},
   "source": [
    "### Question 2 (Math)\n",
    "\n",
    "Hence, we consider the worst-case convergence bound by solving:\n",
    "\\begin{eqnarray*}\n",
    "P^*_k = \\arg\\min_{P\\in\\mathcal{P}_k}\\max_{\\lambda\\in[\\mu, L]}|P(\\lambda)|,\n",
    "\\end{eqnarray*}\n",
    "where $\\mathcal{P}_k$ is the set of polynomials with degree at most $k$ and with $P(0)=1$.\n",
    "\n",
    "$P^*_k$ are derived from Chebyshev polynomials of the first kind. Chebyshev polynomials of the first kind are defined recursively as follows:\n",
    "\\begin{eqnarray*}\n",
    "C_0(x) &=& 1,\\\\\n",
    "C_1(x) &=&x,\\\\\n",
    "C_{k}(x) &=& 2xC_{k-1}(x)-C_{k-2}(x), \\text{ for } k\\geq 2.\n",
    "\\end{eqnarray*}\n",
    "We know that they satisfy the minimax property:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{1}{2^{k-1}} C_k = \\arg\\min_{P\\in \\mathcal{P}'_k} \\max_{x\\in [-1,1]} |P(x)|,\n",
    "\\end{eqnarray*}\n",
    "where $\\mathcal{P}'_k$ is the set of monic polynomials with degree less than $k$.\n",
    "A monic polynomial is a polynomial whose coefficient associated with the highest power is equal to one.\n",
    "\n",
    "In oder to solve our original problem, we see that we will need to **shift** the interval $[-1,1]$ to $[\\mu,L]$ and to **rescale** the polyomial to get $P(0)=1$. This is done thanks to the linear mapping\n",
    "\\begin{eqnarray*}\n",
    "t^{[\\mu,L]}: [\\mu, L] &\\to& [-1,1]\\\\\n",
    "x&\\mapsto& \\frac{2x-(L+\\mu)}{L-\\mu}\n",
    "\\end{eqnarray*}\n",
    "and then\n",
    "\\begin{eqnarray*}\n",
    "C^{[\\mu,L]}_k(x) = \\frac{C_k(t^{[\\mu,L]}(x))}{C_k(t^{[\\mu,L]}(0)}.\n",
    "\\end{eqnarray*}\n",
    "Using the recursion defining $C_k$, we get:\n",
    "\\begin{eqnarray*}\n",
    "C^{[\\mu,L]}_0(x)&=& 1,\\\\\n",
    "C^{[\\mu,L]}_1(x)&=&1-\\frac{2}{L+\\mu}x,\\\\\n",
    "C^{[\\mu,L]}_k(x) &=& \\frac{2\\delta_k}{L-\\mu}(L+\\mu-2x) C^{[\\mu,L]}_{k-1}(x) +\\left( 1-\\frac{2\\delta_k(L+\\mu)}{L-\\mu}\\right)C^{[\\mu,L]}_{k-2}(x), \\text{ for } k\\geq 2,\n",
    "\\end{eqnarray*}\n",
    "with $\\delta_1=\\frac{L-\\mu}{L+\\mu}$ and for $k\\geq 2$,\n",
    "\\begin{eqnarray*}\n",
    "\\delta_k = \\left( 2\\frac{L+\\mu}{L-\\mu}-\\delta_{k-1}\\right)^{-1}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Show that the resulting algorithm can be written as:\n",
    "\\begin{eqnarray*}\n",
    "x_k = x_{k-1} -\\frac{4\\delta_k}{L-\\mu}\\nabla f(x_{k-1}) + \\left( 1-2\\delta_k\\frac{L+\\mu}{L-\\mu}\\right)(x_{k-2}-x_{k-1}).\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ff679",
   "metadata": {},
   "source": [
    "**solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771ba67",
   "metadata": {},
   "source": [
    "### Question 3 (Code)\n",
    "\n",
    "Code the Chebyshev method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc740cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chebyshev_MomemtumGradientDescent(f,grad,L,mu,x0,maxiter=1e3):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cmgd, xhist_cmgd = Chebyshev_MomemtumGradientDescent(f,grad,L,mu,x0,maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b6b52",
   "metadata": {},
   "source": [
    "Compare Chebyshev method to vanilla Gradient descent (the code below should run without any modification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.semilogy( xhist_cmgd, label='Chebyshev method' )\n",
    "plt.semilogy( xhist_gdopti, label='Gradient Descent (Opti)' )\n",
    "plt.ylim(bottom=1e-13,top=1e2)\n",
    "k = np.arange(1,maxiter)\n",
    "plt.semilogy(k,((kappa-1)/(kappa+1))**k,':',label='$((\\kappa-1)/(\\kappa+1))^{k}$')\n",
    "plt.semilogy(k,((np.sqrt(kappa)-1)/(np.sqrt(kappa)+1))**k,':',label='$((\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1))^{k}$')\n",
    "plt.legend( loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726a3d4",
   "metadata": {},
   "source": [
    "### Question 4 (Maths)\n",
    "\n",
    "What is the limit for $\\delta_k$ when $k\\to\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ae56e",
   "metadata": {},
   "source": [
    "**solution :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b51f6",
   "metadata": {},
   "source": [
    "Show that the corresponding recursion when $\\delta_k$ is replaced by $\\delta_\\infty$ in Chebyshev method is:\n",
    "\\begin{eqnarray*}\n",
    "x_k = x_{k-1} - \\frac{4}{(\\sqrt{L}+\\sqrt{\\mu})^2}\\nabla f(x_{k-1}) +\\frac{(\\sqrt{L}-\\sqrt{\\mu})^2}{(\\sqrt{L}+\\sqrt{\\mu})^2}(x_{k-1}-x_{k-2}),\n",
    "\\end{eqnarray*}\n",
    "which corresponds to Polyak's heavy-ball method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a13719",
   "metadata": {},
   "source": [
    "### Question 5 (Code)\n",
    "\n",
    "Code the Polyak's heavy-ball method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MomemtumGradientDescent(f,grad,x0,maxiter=1e3):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21debb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mgd, xhist_mgd = MomemtumGradientDescent(f,grad,x0,maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337979c",
   "metadata": {},
   "source": [
    "Compare all the methods (the code below should run without any modification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.semilogy( xhist_cmgd, label='Chebyshev method' )\n",
    "plt.semilogy( xhist_gdopti, label='Gradient Descent (Opti)' )\n",
    "plt.semilogy( xhist_mgd, label='Polyak method' )\n",
    "k = np.arange(1,maxiter)\n",
    "plt.semilogy(k,((kappa-1)/(kappa+1))**k,':',label='$((\\kappa-1)/(\\kappa+1))^{k}$')\n",
    "plt.semilogy(k,((np.sqrt(kappa)-1)/(np.sqrt(kappa)+1))**k,':',label='$((\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1))^{k}$')\n",
    "plt.ylim(bottom=1e-13,top=1e2)\n",
    "plt.legend( loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-icfp",
   "language": "python",
   "name": "ml-icfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
