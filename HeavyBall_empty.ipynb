{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d022471-893a-4ac2-a037-4f8e18f63b1f",
   "metadata": {},
   "source": [
    "## Optimization problem\n",
    "\n",
    "We consider the ordinary least-squares problem (studied in the course) with the following notations:\n",
    "\\begin{eqnarray*}\n",
    "A\\in \\mathbb{R}^{n\\times d}, \\quad x^* = 1\\in \\mathbb{R}^d, \\quad b = Ax^*\\\\\n",
    "f(x) =\\frac{1}{2}\\| Ax-b\\|^2.\n",
    "\\end{eqnarray*}\n",
    "Least-squares estimation amounts to finding a minimizer of $f$: $f^* =\\min_x f(x) =0$.\n",
    "\n",
    "In the course, we analyzed the gradient descent algorithm. Here we have:\n",
    "\\begin{eqnarray*}\n",
    "\\nabla f(x) = A^T(Ax-b),\n",
    "\\end{eqnarray*}\n",
    "so that we can write the algorithm as:\n",
    "\\begin{eqnarray*}\n",
    "x_0 &=& 0\\\\\n",
    "x_{k+1} &=& x_k-\\gamma A^T(Ax_k-b).\n",
    "\\end{eqnarray*}\n",
    "The Hessian matrix $H = A^TA$ play a crucial role. $H$ belongs to the set of $d\\times d$ symmetric matrices $\\mathcal{S}_d$. We define $\\mu=\\lambda_{\\min}(H)$ and $L=\\lambda_\\max(H)$ so that $\\kappa=\\frac{L}{\\mu}$ is the condition number.\n",
    "\n",
    "In the course, we showed the closed form expression:\n",
    "\\begin{eqnarray*}\n",
    "x_{k+1} - x^* &=& x_k-x^* -\\gamma A^T A(x_k-x^*)\\\\\n",
    "x_{k+1} - x^* &=& \\left( I- \\gamma A^TA \\right)(x_k-x^*)\\\\\n",
    "&=& \\left( I- \\gamma H \\right)^k(x_0-x^*).\n",
    "\\end{eqnarray*}\n",
    "for $H x^* = A^Tb$ solution of $\\nabla f(x) =0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng   = np.random.default_rng(1)\n",
    "m   = 55\n",
    "n   = 50\n",
    "A   = rng.normal( size=(m,n) )\n",
    "xStar = np.ones( (n,1) )\n",
    "b   = A@xStar\n",
    "\n",
    "f   = lambda x : norm(A@x-b)**2/2\n",
    "grad= lambda x : A.T@( A@x-b )\n",
    "\n",
    "fStar   = 0\n",
    "x0      = np.zeros((n,1))\n",
    "\n",
    "evals = np.linalg.eigvals(A.T@A)\n",
    "L     = np.max(evals)\n",
    "mu    = np.min(evals)\n",
    "kappa = L/mu\n",
    "print(f\"L is {L:.2f}, mu is {mu:.2f}, condition number is {kappa:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(f,grad,stepsize,x0,maxiter=1e3):\n",
    "    x = x0.copy()\n",
    "    fHist = []\n",
    "    for k in range(int(maxiter)):\n",
    "        x -= stepsize*grad(x)\n",
    "        fHist.append( f(x) )\n",
    "    return x, fHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d74e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 1e4\n",
    "x_gd, fHist_gd  = gradientDescent(f,grad,1/L,x0,maxiter=maxiter)\n",
    "\n",
    "x_gdopti, fHist_gdopti  = gradientDescent(f,grad,2/(L+mu),x0,maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.semilogy( fHist_gd, label='Gradient Descent (Naive)' )\n",
    "plt.semilogy( fHist_gdopti, label='Gradient Descent (Opti)' )\n",
    "k = np.arange(1,maxiter)\n",
    "plt.semilogy(k,1e-4*(1-1/kappa)**k,':',label='$(1-\\\\kappa^{-1})^{k}$')\n",
    "plt.legend( loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bc471",
   "metadata": {},
   "source": [
    "Code the Heavy Ball method:\n",
    "$$x^{(k+1)} = x^{(k)} - \\alpha \\nabla f( x^{(k)}) + \\beta (x^{(k)} - x^{(k-1)}) \\qquad \\text{ for some constants } \\alpha>0, \\beta\\geq 0 . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MomemtumGradientDescent(f,grad,stepsize,momentum,x0,maxiter=1e3):\n",
    "    x = x0.copy()\n",
    "    fHist = []\n",
    "    # your code here\n",
    "    return x, fHist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d57042",
   "metadata": {},
   "source": [
    "To compute the optimal choices for the stepsize and momentum $\\alpha$ and $\\beta$, we start by looking at the recursion:\n",
    "\\begin{eqnarray*}\n",
    "\\left(\\begin{matrix}\n",
    "x^{(k+1)}-x^*\\\\\n",
    "x^{(k)}-x^*\n",
    "\\end{matrix}\\right) &=& \\left(\\begin{matrix} x^{(k)} - \\alpha \\nabla f( x^{(k)}) + \\beta (x^{(k)} - x^{(k-1)}) - x^*\\\\\n",
    "x^{(k)}-x^*\\end{matrix}\\right)\\\\\n",
    "&=& \\left(\\begin{matrix} (1+\\beta)I -\\alpha H & -\\beta I\\\\\n",
    "I & 0\\end{matrix}\\right)\\left(\\begin{matrix} x^{(k)}  - x^*\\\\\n",
    "x^{(k-1)} -x^*\\end{matrix}\\right)\n",
    "\\end{eqnarray*}\n",
    "To get a simpler matrix to analyze, we can \"uncouple\" the equations as follows: diagonalize $H = V\\Lambda V^T$ with $VV^T=I$ and $\\Lambda$ the diagonal matrix of the eigenvalues of $H$. Define $w^{(k)} = V^T(x^{(k)}-x^*)$ so that we get:\n",
    "\\begin{eqnarray*}\n",
    "w^{(k+1)} = w^{(k)} - \\alpha ( V^T Hx^{(k)} - V^T Hx^* ) +\\beta ( V^T x^{(k)} - V^T x^{(k-1)}),\n",
    "\\end{eqnarray*}\n",
    "and since $V^TH = \\Lambda V^T$, we get:\n",
    "\\begin{eqnarray*}\n",
    "w^{(k+1)} = ((1+\\beta)-\\alpha\\Lambda) w^{(k)} -\\beta w^{(k-1)}.\n",
    "\\end{eqnarray*}\n",
    "For the $i$-th component of $w^{(k+1)}$, we need to look at the equation $u^2-(1+\\beta-\\alpha \\lambda_i) u +\\beta=0$ and make sure that the solutions have a module less than one. A necessary and sufficient condition is:\n",
    "\\begin{eqnarray*}\n",
    "-1<&\\beta&<1\\\\\n",
    "0<&\\alpha \\lambda_i& <2+2\\beta.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Show that the choice $\\alpha=\\frac{4}{(\\sqrt{L}+\\sqrt{\\ell})^2}$ and $\\beta = \\frac{\\sqrt{L}-\\sqrt{\\ell}}{\\sqrt{L}+\\sqrt{\\ell}}$ is valid and give the corresponding rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887593bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d6a4b",
   "metadata": {},
   "source": [
    "What happens when momentum is too high? too low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50503b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97febee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.semilogy( fHist_gd, label='Gradient Descent (Naive)' )\n",
    "plt.semilogy( fHist_gdopti, label='Gradient Descent (Opti)' )\n",
    "\n",
    "k = np.arange(1,maxiter)\n",
    "plt.semilogy(k,1e-4*(1-1/kappa)**k,':',label='$(1-\\\\kappa^{-1})^{k}$')\n",
    "plt.semilogy(k,1e3*((np.sqrt(kappa)-1)/(np.sqrt(kappa)+1))**k,':',label='$((\\\\kappa^{1/2}-1)/(\\\\kappa^{1/2}+1))^{k}$')\n",
    "plt.ylim(bottom=1e-29,top=1e2)\n",
    "plt.legend( loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a674d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
